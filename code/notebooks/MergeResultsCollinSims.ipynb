{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, re, pickle, os, pandas, rpy2, time, math, yaml, numpy, sklearn\n",
    "import rpy2.robjects as ro\n",
    "from odict import odict\n",
    "from rpy2.robjects.packages import importr\n",
    "calR = importr('CalibrationCurves')\n",
    "grdevices = importr('grDevices')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Choose the Experiment Set\n",
    "\n",
    "Choose the top directory of the experiments from which you want to plot the results.\n",
    "Also, define an output directory to which to write the tables and plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define in and output directories\n",
    "\n",
    "top_prefix=\"/hpc/shared/julius_te/tleeuwenberg/collin-out/\"\n",
    "top_level_experiments_output_dir=\"/sims-out/\"\n",
    "top_level_out_dir=\"../../../tables_and_plots/\"\n",
    "#top_level_out_dir = \"/sims-13-11-2020/\"\n",
    "models_to_include = ['LR','Lasso','Ridge','ElasticNet','PCLR','LAELR','Dropout','LRnn']\n",
    "metrics_to_include = ['C (ROC)','Slope','R2','ExpJacc','ExpSE','Intercept','MSE','logMSE']\n",
    "coeff_metrics_to_include = ['sum_neg_oar','sum_pos_oar','perc_oar_neg','perc_oar_pos']\n",
    "CI=0.95\n",
    "# Metrics could be: 'AUROC','R2', 'Brier', 'ScaledBrier', 'Cintercept', 'Cslope', 'ECI'\n",
    "CI_dev = (1.0 - CI) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "oar_coeff_substrings = [\"Submandibular_\", \"Parotid_\", \"PCM_\",\"Supraglottic_\", \"OralCavity_\", \"GlotticArea_\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extracting Results\n",
    "\n",
    "The code below extracts the relevant information from the underlying subdirectories and output files (may take a few minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 800\n",
      "1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ,10 ,11 ,12 ,13 ,14 ,15 ,16 ,17 ,18 ,19 ,20 ,21 ,22 ,23 ,24 ,25 ,26 ,27 ,28 ,29 ,30 ,31 ,32 ,33 ,34 ,35 ,36 ,37 ,38 ,39 ,40 ,41 ,42 ,43 ,44 ,45 ,46 ,47 ,48 ,49 ,50 ,51 ,52 ,53 ,54 ,55 ,56 ,57 ,58 ,59 ,60 ,61 ,62 ,63 ,64 ,65 ,66 ,67 ,68 ,69 ,70 ,71 ,72 ,73 ,74 ,75 ,76 ,77 ,78 ,79 ,80 ,81 ,82 ,83 ,84 ,85 ,86 ,87 ,88 ,89 ,90 ,91 ,92 ,93 ,94 ,95 ,96 ,97 ,98 ,99 ,100 ,101 ,102 ,103 ,104 ,105 ,106 ,107 ,108 ,109 ,110 ,111 ,112 ,113 ,114 ,115 ,116 ,117 ,118 ,119 ,120 ,121 ,122 ,123 ,124 ,125 ,126 ,127 ,128 ,129 ,130 ,131 ,132 ,133 ,134 ,135 ,136 ,137 ,138 ,139 ,140 ,141 ,142 ,143 ,144 ,145 ,146 ,147 ,148 ,149 ,150 ,151 ,152 ,153 ,154 ,155 ,156 ,157 ,158 ,159 ,160 ,161 ,162 ,163 ,164 ,165 ,166 ,167 ,168 ,169 ,170 ,171 ,172 ,173 ,174 ,175 ,176 ,177 ,178 ,179 ,180 ,181 ,182 ,183 ,184 ,185 ,186 ,187 ,188 ,189 ,190 ,191 ,192 ,193 ,194 ,195 ,196 ,197 ,198 ,199 ,200 ,201 ,202 ,203 ,204 ,205 ,206 ,207 ,208 ,209 ,210 ,211 ,212 ,213 ,214 ,215 ,216 ,217 ,218 ,219 ,220 ,221 ,222 ,223 ,224 ,225 ,226 ,227 ,228 ,229 ,230 ,231 ,232 ,233 ,234 ,235 ,236 ,237 ,238 ,239 ,240 ,241 ,242 ,243 ,244 ,245 ,246 ,247 ,248 ,249 ,250 ,251 ,252 ,253 ,254 ,255 ,256 ,257 ,258 ,259 ,260 ,261 ,262 ,263 ,264 ,265 ,266 ,267 ,268 ,269 ,270 ,271 ,272 ,273 ,274 ,275 ,276 ,277 ,278 ,279 ,280 ,281 ,282 ,283 ,284 ,285 ,286 ,287 ,288 ,289 ,290 ,291 ,292 ,293 ,294 ,295 ,296 ,297 ,298 ,299 ,300 ,301 ,302 ,303 ,304 ,305 ,306 ,307 ,308 ,309 ,310 ,311 ,312 ,313 ,314 ,315 ,316 ,317 ,318 ,319 ,320 ,321 ,322 ,323 ,324 ,325 ,326 ,327 ,328 ,329 ,330 ,331 ,332 ,333 ,334 ,335 ,336 ,337 ,338 ,339 ,340 ,341 ,342 ,343 ,344 ,345 ,346 ,347 ,348 ,349 ,350 ,351 ,352 ,353 ,354 ,355 ,356 ,357 ,358 ,359 ,360 ,361 ,362 ,363 ,364 ,365 ,366 ,367 ,368 ,369 ,370 ,371 ,372 ,373 ,374 ,375 ,376 ,377 ,378 ,379 ,380 ,381 ,382 ,383 ,384 ,385 ,386 ,387 ,388 ,389 ,390 ,391 ,392 ,393 ,394 ,395 ,396 ,397 ,398 ,399 ,400 ,401 ,402 ,403 ,404 ,405 ,406 ,407 ,408 ,409 ,410 ,411 ,412 ,413 ,414 ,415 ,416 ,417 ,418 ,419 ,420 ,421 ,422 ,423 ,424 ,425 ,426 ,427 ,428 ,429 ,430 ,431 ,432 ,433 ,434 ,435 ,436 ,437 ,438 ,439 ,440 ,441 ,442 ,443 ,444 ,445 ,446 ,447 ,448 ,449 ,450 ,451 ,452 ,453 ,454 ,455 ,456 ,457 ,458 ,459 ,460 ,461 ,462 ,463 ,464 ,465 ,466 ,467 ,468 ,469 ,470 ,471 ,472 ,473 ,474 ,475 ,476 ,477 ,478 ,479 ,480 ,481 ,482 ,483 ,484 ,485 ,486 ,487 ,488 ,489 ,490 ,491 ,492 ,493 ,494 ,495 ,496 ,497 ,498 ,499 ,500 ,501 ,502 ,503 ,504 ,505 ,506 ,507 ,508 ,509 ,510 ,511 ,512 ,513 ,514 ,515 ,516 ,517 ,518 ,519 ,520 ,521 ,522 ,523 ,524 ,525 ,526 ,527 ,528 ,529 ,530 ,531 ,532 ,533 ,534 ,535 ,536 ,537 ,538 ,539 ,540 ,541 ,542 ,543 ,544 ,545 ,546 ,547 ,548 ,549 ,550 ,551 ,552 ,553 ,554 ,555 ,556 ,557 ,558 ,559 ,560 ,561 ,562 ,563 ,564 ,565 ,566 ,567 ,568 ,569 ,570 ,571 ,572 ,573 ,574 ,575 ,576 ,577 ,578 ,579 ,580 ,581 ,582 ,583 ,584 ,585 ,586 ,587 ,588 ,589 ,590 ,591 ,592 ,593 ,594 ,595 ,596 ,597 ,598 ,599 ,600 ,601 ,602 ,603 ,604 ,605 ,606 ,607 ,608 ,609 ,610 ,611 ,612 ,613 ,614 ,615 ,616 ,617 ,618 ,619 ,620 ,621 ,622 ,623 ,624 ,625 ,626 ,627 ,628 ,629 ,630 ,631 ,632 ,633 ,634 ,635 ,636 ,637 ,638 ,639 ,640 ,641 ,642 ,643 ,644 ,645 ,646 ,647 ,648 ,649 ,650 ,651 ,652 ,653 ,654 ,655 ,656 ,657 ,658 ,659 ,660 ,661 ,662 ,663 ,664 ,665 ,666 ,667 ,668 ,669 ,670 ,671 ,672 ,673 ,674 ,675 ,676 ,677 ,678 ,679 ,680 ,681 ,682 ,683 ,684 ,685 ,686 ,687 ,688 ,689 ,690 ,691 ,692 ,693 ,694 ,695 ,696 ,697 ,698 ,699 ,700 ,701 ,702 ,703 ,704 ,705 ,706 ,707 ,708 ,709 ,710 ,711 ,712 ,713 ,714 ,715 ,716 ,717 ,718 ,719 ,720 ,721 ,722 ,723 ,724 ,725 ,726 ,727 ,728 ,729 ,730 ,731 ,732 ,733 ,734 ,735 ,736 ,737 ,738 ,739 ,740 ,741 ,742 ,743 ,744 ,745 ,746 ,747 ,748 ,749 ,750 ,751 ,752 ,753 ,754 ,755 ,756 ,757 ,758 ,759 ,760 ,761 ,762 ,763 ,764 ,765 ,766 ,767 ,768 ,769 ,770 ,771 ,772 ,773 ,774 ,775 ,776 ,777 ,778 ,779 ,780 ,781 ,782 ,783 ,784 ,785 ,786 ,787 ,788 ,789 ,790 ,791 ,792 ,793 ,794 ,795 ,796 ,797 ,798 ,799 ,800 ,\n",
      "\n",
      "Experiments: 8\n",
      "CompareMethods:sim-A ( 100 bs )\n",
      "CompareMethods:sim-A' ( 100 bs )\n",
      "CompareMethods:sim-B ( 100 bs )\n",
      "CompareMethods:sim-B' ( 100 bs )\n",
      "CompareMethods:sim-C ( 100 bs )\n",
      "CompareMethods:sim-C' ( 100 bs )\n",
      "CompareMethods:sim-D ( 100 bs )\n",
      "CompareMethods:sim-D' ( 100 bs )\n"
     ]
    }
   ],
   "source": [
    "# relative paths to locate relevant files\n",
    "search_path = top_prefix+top_level_experiments_output_dir+\"/\"\n",
    "bootrap_path_extension = \"/**/bs*/*\"\n",
    "model_preds_path = '/pred_probs/model_preds.pickle'\n",
    "true_labels_path= '/pred_probs/true_labels.pickle'\n",
    "gt_model_path = '/sim/sim_gt_model.p'\n",
    "test_probs = '/sim/test_probs.p'\n",
    "train_data = '/sim/train_data.p'\n",
    "rval_path ='/rvals/rvals_per_model.p'\n",
    "\n",
    "model_coef_path = '/models.csv'\n",
    "trained_models_path='/trained_models.p'\n",
    "yaml_config_path = '/../../../config.yml'\n",
    "\n",
    "results = odict()\n",
    "       \n",
    "        # --- Results Structure ---\n",
    "        # results > $exp > preds > $model_name > $bs > $pred_probs\n",
    "        # results  > $exp > labs > $bs > $true_labs\n",
    "        # results > $exp > models > $model_name > $coeffs > $bs > $coef_value\n",
    "        # results > $exp > bs_ixs > $bootstrap_indices\n",
    "        # results > $exp > yaml > $yaml_dict\n",
    "print('total:',len(list(glob.glob(search_path + bootrap_path_extension, recursive=True))))\n",
    "i=0        \n",
    "for bs_path in glob.glob(search_path + bootrap_path_extension, recursive=True):\n",
    "    i+=1\n",
    "    # extract experiment name and bootstrap number from the path\n",
    "    exp_name = bs_path.split('/')[-1]\n",
    "    bs = int(re.match(r'.*/bs-(\\d+)/.*',bs_path).groups()[0])\n",
    "    print(str(i), end = ' ,')\n",
    "    #print('>',exp_name, ' bootrap:', bs)\n",
    "    \n",
    "    # setup exp dict\n",
    "    if not exp_name in results:\n",
    "        results[exp_name] = {'bs_path':{}, 'preds':{},'labs':{}, 'models':{},'gt_models':{},'gt_test_probs':{}, 'bs_ixs':[],'rvals':{},'pymodels':{},'train_labs':{}}\n",
    "\n",
    "    results[exp_name]['bs_path'][bs] = bs_path\n",
    "    \n",
    "    # get the bs index\n",
    "    if not bs in results[exp_name]['bs_ixs']:\n",
    "        results[exp_name]['bs_ixs'].append(bs)\n",
    "    \n",
    "    # get the yaml config\n",
    "    if os.path.exists(bs_path + yaml_config_path):\n",
    "        with open(bs_path + yaml_config_path) as config_file:\n",
    "            results[exp_name]['yaml'] = yaml.full_load(config_file)\n",
    "    \n",
    "    # Get the model PREDICTIONS (per bs per model) - if available in model_preds_path\n",
    "    if os.path.exists(bs_path + model_preds_path):\n",
    "        with open(bs_path + model_preds_path, 'rb') as f:\n",
    "            for model_name, model_predictions in pickle.load(f).items():\n",
    "                if not model_name in results[exp_name]['preds']:\n",
    "                    results[exp_name]['preds'][model_name] = {}\n",
    "                results[exp_name]['preds'][model_name][bs] = [float(p) for p in model_predictions]\n",
    "    \n",
    "    # Get the true LABELS (per bs) - if available in true_labels_path\n",
    "    if os.path.exists(bs_path + true_labels_path):\n",
    "        with open(bs_path + true_labels_path, 'rb') as f:\n",
    "            if not bs in results[exp_name]['labs']:\n",
    "                results[exp_name]['labs'][bs] = pickle.load(f)\n",
    "            else:\n",
    "                # if there is already a set of true labels for this bs and exp, check if they are the same\n",
    "                if not pickle.load(f) == results[exp_name]['labs'][bs]:\n",
    "                    print('WARNING: labels are not consistent between models for exp', exp_name, ' bs', bs)\n",
    "    \n",
    "    # Get the already validated predictions\n",
    "    if os.path.exists(bs_path + rval_path):\n",
    "        with open(bs_path + rval_path, 'rb') as f:\n",
    "            for model_name, model_rval in pickle.load(f).items():\n",
    "                    if not model_name in results[exp_name]['rvals']:\n",
    "                        results[exp_name]['rvals'][model_name] = {}\n",
    "                    results[exp_name]['rvals'][model_name][bs] = model_rval\n",
    "\n",
    "    # Get the model coefficients (per bs per model)\n",
    "    if os.path.exists(bs_path + model_coef_path):\n",
    "        models_coeff_df = pandas.read_csv(bs_path + model_coef_path, sep='\\t',index_col=0, header=0).filter(regex=r\".*_mean\")\n",
    "        \n",
    "        # get coef names (columns with _mean in it)\n",
    "        coef_names = models_coeff_df.columns\n",
    "\n",
    "        # get the coefficient values\n",
    "        for row in models_coeff_df.iterrows():\n",
    "            model_name = row[0].split('-')[-1]\n",
    "            if not model_name in results[exp_name]['models']:\n",
    "                results[exp_name]['models'][model_name] = {coef_name:{} for coef_name in coef_names}\n",
    "            for coef_name in results[exp_name]['models'][model_name]:\n",
    "                results[exp_name]['models'][model_name][coef_name][bs] = row[1][coef_name]\n",
    "\n",
    "                \n",
    "    # SIMULATION \n",
    "    # Get GT model \n",
    "    if os.path.exists(bs_path + gt_model_path):\n",
    "        with open(bs_path + gt_model_path, 'rb') as f:\n",
    "            results[exp_name]['gt_models'][bs] = pickle.load(f)\n",
    "\n",
    "    # Get Simulated test data \n",
    "    if os.path.exists(bs_path + test_probs):\n",
    "        with open(bs_path + test_probs, 'rb') as f:\n",
    "            results[exp_name]['gt_test_probs'][bs] = pickle.load(f)\n",
    "\n",
    "    # Train data\n",
    "    if os.path.exists(bs_path + train_data):\n",
    "        with open(bs_path + train_data, 'rb') as f:\n",
    "            train_df = pickle.load(f).filter(regex=(\"^y_*\"))\n",
    "            #print(train_df.keys())\n",
    "            results[exp_name]['train_labs'][bs] = list(train_df.values)                       \n",
    "            \n",
    "# Plotting overview\n",
    "print('\\n\\nExperiments:',len(results))\n",
    "for exp in sorted(results):\n",
    "    print(exp, '(', len(results[exp]['bs_ixs']),'bs )')\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting info on simulation settings (GT AUROC, no. predictors, no. events, EPV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp\tAUROC\tPreds\tEvents\tEPV\n",
      "A\t0.79\t7\t159.54\t22.79\n",
      "A'\t0.8\t7\t160.14\t22.88\n",
      "B\t0.79\t19\t159.64\t8.4\n",
      "B'\t0.8\t19\t160.03\t8.42\n",
      "C\t0.87\t13\t83.33\t6.41\n",
      "C'\t0.87\t13\t83.0\t6.38\n",
      "D\t0.85\t43\t83.54\t1.94\n",
      "D'\t0.85\t43\t83.59\t1.94\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('\\t'.join(['Exp','AUROC','Preds','Events','EPV']))\n",
    "sorted_exps = sorted(list(results.keys()))\n",
    "for exp_name in sorted_exps:\n",
    "    \n",
    "    gt_aurocs = []\n",
    "    no_events = []\n",
    "    no_preds = len(results[exp_name]['gt_models'][0].best_estimator_.coef_[0])\n",
    "    epv = []\n",
    "    for bs in results[exp_name]['gt_test_probs']:\n",
    "        \n",
    "        if bs in results[exp_name]['labs']:\n",
    "            auroc = sklearn.metrics.roc_auc_score(results[exp_name]['labs'][bs],results[exp_name]['gt_test_probs'][bs])\n",
    "            gt_aurocs.append(auroc)\n",
    "            no_events.append(sum(results[exp_name]['train_labs'][bs]))\n",
    "            \n",
    "            epv.append(sum(results[exp_name]['train_labs'][bs]) / no_preds)\n",
    "            #print(results[exp_name]['train_labs'][bs])\n",
    "    print('\\t'.join([exp_name.replace('CompareMethods:sim-',''),str(round(numpy.mean(gt_aurocs),2)),str(no_preds),str(round(numpy.mean(no_events),2)),str(round(numpy.mean(epv),2))]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics_to_include = ['C (ROC)','Slope','R2','ExpJacc','ExpSE','Intercept','logMSE'] # remove MSE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Calculating predictive performance evaluations\n",
    "May take a few minutes...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompareMethods:sim-A'\n",
      "7, 65, 68, 99, 60, 66, 31, 72, 84, 13, 79, 53, 41, 9, 61, 49, 86, 25, 16, 88, 94, 80, 40, 52, 51, 24, 54, 3, 26, 89, 11, 71, 74, 23, 8, 46, 81, 95, 37, 82, 93, 32, 36, 58, 47, 73, 78, 2, 97, 44, 42, 22, 98, 64, 75, 76, 39, 28, 92, 12, 6, 63, 15, 29, 87, 67, 20, 34, 55, 21, 35, 62, 30, 1, 83, 4, 59, 10, 57, 90, 50, 85, 27, 5, 43, 91, 56, 17, 38, 33, 45, 70, 96, 18, 77, 69, 48, 14, 0, 19, calculating CI\n",
      "CompareMethods:sim-D\n",
      "7, 65, 68, 99, 60, 66, 31, 72, 84, 13, 79, 53, 41, 9, 61, 49, 86, 25, 16, 88, 94, 80, 40, 52, 51, 24, 54, 3, 26, 89, 11, 71, 74, 23, 8, 46, 81, 95, 37, 82, 93, 32, 36, 58, 47, 73, 78, 2, 97, 44, 42, 22, 98, 64, 75, 76, 39, 28, 92, 12, 6, 63, 15, 29, 87, 67, 20, 34, 55, 21, 35, 62, 30, 1, 83, 4, 59, 10, 57, 90, 50, 85, 27, 5, 43, 91, 56, 17, 38, 33, 45, 70, 96, 18, 77, 69, 48, 14, 0, 19, calculating CI\n",
      "CompareMethods:sim-C\n",
      "7, 65, 68, 99, 60, 66, 31, 72, 84, 13, 79, 53, 41, 9, 61, 49, 86, 25, 16, 88, 94, 80, 40, 52, 51, 24, 54, 3, 26, 89, 11, 71, 74, 23, 8, 46, 81, 95, 37, 82, 93, 32, 36, 58, 47, 73, 78, 2, 97, 44, 42, 22, 98, 64, 75, 76, 39, 28, 92, 12, 6, 63, 15, 29, 87, 67, 20, 34, 55, 21, 35, 62, 30, 1, 83, 4, 59, 10, 57, 90, 50, 85, 27, 5, 43, 91, 56, 17, 38, 33, 45, 70, 96, 18, 77, 69, 48, 14, 0, 19, calculating CI\n",
      "CompareMethods:sim-C'\n",
      "7, 65, 68, 99, 60, 66, 31, 72, 84, 13, 79, 53, 41, 9, 61, 49, 86, 25, 16, 88, 94, 80, 40, 52, 51, 24, 54, 3, 26, 89, 11, 71, 74, 23, 8, 46, 81, 95, 37, 82, 93, 32, 36, 58, 47, 73, 78, 2, 97, 44, 42, 22, 98, 64, 75, 76, 39, 28, 92, 12, 6, 63, 15, 29, 87, 67, 20, 34, 55, 21, 35, 62, 30, 1, 83, 4, 59, 10, 57, 90, 50, 85, 27, 5, 43, 91, 56, 17, 38, 33, 45, 70, 96, 18, 77, 69, 48, 14, 0, 19, calculating CI\n",
      "CompareMethods:sim-D'\n",
      "7, 65, 68, 99, 60, 66, 31, 72, 84, 13, 79, 53, 41, 9, 61, 49, 86, 25, 16, 88, 94, 80, 40, 52, 51, 24, 54, 3, 26, 89, 11, 71, 74, 23, 8, 46, 81, 95, 37, 82, 93, 32, 36, 58, 47, 73, 78, 2, 97, 44, 42, 22, 98, 64, 75, 76, 39, 28, 92, 12, 6, 63, 15, 29, 87, 67, 20, 34, 55, 21, 35, 62, 30, 1, 83, 4, 59, 10, 57, 90, 50, 85, 27, 5, 43, 91, 56, 17, 38, 33, 45, 70, 96, 18, 77, 69, 48, 14, 0, 19, calculating CI\n",
      "CompareMethods:sim-A\n",
      "7, 65, 68, 99, 60, 66, 31, 72, 84, 13, 79, 53, 41, 9, 61, 49, 86, 25, 16, 88, 94, 80, 40, 52, 51, 24, 54, 3, 26, 89, 11, 71, 74, 23, 8, 46, 81, 95, 37, 82, 93, 32, 36, 58, 47, 73, 78, 2, 97, 44, 42, 22, 98, 64, 75, 76, 39, 28, 92, 12, 6, 63, 15, 29, 87, 67, 20, 34, 55, 21, 35, 62, 30, 1, 83, 4, 59, 10, 57, 90, 50, 85, 27, 5, 43, 91, 56, 17, 38, 33, 45, 70, 96, 18, 77, 69, 48, 14, 0, 19, calculating CI\n",
      "CompareMethods:sim-B'\n",
      "7, 65, 68, 99, 60, 66, 31, 72, 84, 13, 79, 53, 41, 9, 61, 49, 86, 25, 16, 88, 94, 80, 40, 52, 51, 24, 54, 3, 26, 89, 11, 71, 74, 23, 8, 46, 81, 95, 37, 82, 93, 32, 36, 58, 47, 73, 78, 2, 97, 44, 42, 22, 98, 64, 75, 76, 39, 28, 92, 12, 6, 63, 15, 29, 87, 67, 20, 34, 55, 21, 35, 62, 30, 1, 83, 4, 59, 10, 57, 90, 50, 85, 27, 5, 43, 91, 56, 17, 38, 33, 45, 70, 96, 18, 77, 69, 48, 14, 0, 19, calculating CI\n",
      "CompareMethods:sim-B\n",
      "7, 65, 68, 99, 60, 66, 31, 72, 84, 13, 79, 53, 41, 9, 61, 49, 86, 25, 16, 88, 94, 80, 40, 52, 51, 24, 54, 3, 26, 89, 11, 71, 74, 23, 8, 46, 81, 95, 37, 82, 93, 32, 36, 58, 47, 73, 78, 2, 97, 44, 42, 22, 98, 64, 75, 76, 39, 28, 92, 12, 6, 63, 15, 29, 87, 67, 20, 34, 55, 21, 35, 62, 30, 1, 83, 4, 59, 10, 57, 90, 50, 85, 27, 5, 43, 91, 56, 17, 38, 33, 45, 70, 96, 18, 77, 69, 48, 14, 0, 19, calculating CI\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Based on CalibrationCurve library in R\n",
    "from sklearn.metrics import jaccard_score\n",
    "import random\n",
    "\n",
    "rval = ro.r['val.prob.ci.2']\n",
    "#R metrics: 'Dxy', 'C (ROC)', 'R2', 'D', 'D:Chi-sq', 'D:p', 'U', 'U:Chi-sq', 'U:p', 'Q', 'Brier', 'Intercept', 'Slope', 'Emax', 'Brier scaled'\n",
    "\n",
    "\n",
    "sign = lambda coeffs: [1.0 if c >=0.01 else -1.0 if c <= -0.01 else 0.0 for c in coeffs]\n",
    "\n",
    "for exp_name in results:\n",
    "    print(exp_name)\n",
    "    #break\n",
    "    \n",
    "    # Setup output dir\n",
    "    plot_path = top_level_out_dir + '/calib_plots/' \n",
    "    if not os.path.exists(os.path.dirname(plot_path)):\n",
    "        os.makedirs(os.path.dirname(plot_path))\n",
    "    \n",
    "    \n",
    "    # 1. Calculate the scores for all bootstraps\n",
    "    scores = {model:{metric:{} for metric in metrics_to_include} for model in models_to_include}\n",
    "    results[exp_name]['inclusion_vectors'] = {model:{} for model in results[exp_name]['models']}\n",
    "    results[exp_name]['true_inclusion_vectors'] = {}\n",
    "        \n",
    "    for bs in results[exp_name]['bs_ixs']:\n",
    "        print(bs,end=', ')    \n",
    "        \n",
    "        if 'sim' in exp_name:\n",
    "            gt_model_coeffs = numpy.array(results[exp_name]['gt_models'][bs].best_estimator_.coef_).flatten()\n",
    "            true_inclusion_vec = sign(gt_model_coeffs)        \n",
    "            results[exp_name]['true_inclusion_vectors'][bs] = true_inclusion_vec\n",
    "        \n",
    "        for model in scores:\n",
    "            # for each model inclusion vecs are constructed and saved\n",
    "            coeff_names = [c for c in results[exp_name]['models'][model].keys() if not c=='Intercept_mean']\n",
    "            inclusion_vec = sign([results[exp_name]['models'][model][coeff_name][bs] for coeff_name in coeff_names])\n",
    "            results[exp_name]['inclusion_vectors'][model][bs] = inclusion_vec\n",
    "            \n",
    "            # for each model performance evaluation metrics are calculated and saved \n",
    "            if bs in results[exp_name]['preds'][model]:\n",
    "                # ! Metrics are calculated here (currently using the val.prob.ci.2' R function)\n",
    "                val = results[exp_name]['rvals'][model][bs]\n",
    "                rval_dict = dict(zip(results[exp_name]['rvals'][model][bs].names, list(results[exp_name]['rvals'][model][bs])))\n",
    "                for metric, v in rval_dict.items():\n",
    "                    if not metric in scores[model]:\n",
    "                        scores[model][metric] = {}\n",
    "                    scores[model][metric][bs] = v\n",
    "                    \n",
    "        # for each bs, the trained models are loaded and MSE is calculated per method\n",
    "        trained_models_path = results[exp_name]['bs_path'][bs] + '/trained_models.p'\n",
    "        if os.path.exists(trained_models_path) and bs in results[exp_name]['gt_models']:\n",
    "            with open(trained_models_path,'rb') as f:\n",
    "                for model_name, trained_models in pickle.load(f).items():\n",
    "                    mses = []\n",
    "                    for trained_model in trained_models:\n",
    "\n",
    "                        model_coeffs = numpy.array(trained_model.best_estimator_.coef_).flatten()\n",
    "                        true_coeffs = numpy.array(results[exp_name]['gt_models'][bs].best_estimator_.coef_).flatten()\n",
    "\n",
    "                        mse = numpy.mean((true_coeffs-model_coeffs)**2)\n",
    "\n",
    "                        mses.append(mse)\n",
    "                    scores[model_name]['MSE'][int(bs)] = numpy.mean(mses)\n",
    "                    scores[model_name]['logMSE'][int(bs)] = numpy.log(numpy.mean(mses))\n",
    "        \n",
    "    \n",
    "    # Approximating the expected overlap in selected coefficients (and has the same sign) when repeating the experiment, for each method.\n",
    "    for model in results[exp_name]['inclusion_vectors']:\n",
    "        scores[model]['ExpJacc'] = {}\n",
    "        scores[model]['ExpSE'] = {}\n",
    "        \n",
    "        incl_vec_list = list(results[exp_name]['inclusion_vectors'][model].values())\n",
    "        \n",
    "        coeffs_per_bs = {bs: numpy.array([results[exp_name]['models'][model][coeff_name][bs] for coeff_name in coeff_names]) for bs in results[exp_name]['bs_ixs']}\n",
    "        mean_coeff = numpy.mean(list(coeffs_per_bs.values()),axis=0)\n",
    "        diffs = [sklearn.metrics.mean_squared_error(coeff,mean_coeff) for coeff in coeffs_per_bs.values()]\n",
    "        \n",
    "        for bs in results[exp_name]['bs_ixs']:\n",
    "            scores[model]['ExpSE'][bs] = numpy.mean(random.choices(diffs, k=len(results[exp_name]['bs_ixs'])))\n",
    "        \n",
    "        for bs in results[exp_name]['bs_ixs']: #100 x\n",
    "            sampled_incl_vec_list = random.choices(incl_vec_list, k=len(results[exp_name]['bs_ixs'])) \n",
    "            sims = []\n",
    "            ses = []\n",
    "            checked = set()\n",
    "            # num_bootstraps x num_bootstraps / 2  \n",
    "            for i,vec1 in enumerate(sampled_incl_vec_list):\n",
    "                for j,vec2 in enumerate(sampled_incl_vec_list):\n",
    "                    \n",
    "                    if not (i==j or (i,j) in checked):\n",
    "                            \n",
    "                            # Mean jacc (based only on included coefficients only)\n",
    "                            intersection_size = sum([1.0 if vec1[i]==vec2[i] and vec1[i]!=0.0 else 0.0 for i in range(len(vec1))])\n",
    "                            union_size = sum([1 if vec1[i]!= 0 or vec2[i]!=0 else 0 for i in range(len(vec1))])\n",
    "\n",
    "                            similarity = intersection_size / union_size\n",
    "                            sims.append(similarity)\n",
    "\n",
    "                            checked.add((i,j))\n",
    "                            checked.add((j,i))\n",
    "                        \n",
    "            scores[model]['ExpJacc'][bs] = numpy.mean(sims)\n",
    "    \n",
    "\n",
    "        \n",
    "            \n",
    "    print('calculating CI')\n",
    "    # 2. Determine the reported scores, and confidence intervals\n",
    "    main_scores, CI_lower, CI_upper = {model:{} for model in models_to_include}, {model:{} for model in models_to_include}, {model:{} for model in models_to_include}\n",
    "    for model in scores:\n",
    "        for metric in metrics_to_include:\n",
    "            sorted_scores = sorted(scores[model][metric].values())\n",
    "            main_scores[model][metric] =  numpy.mean(list(scores[model][metric].values()))\n",
    "\n",
    "            CI_lower_ix = int(CI_dev*len(scores[model][metric]))\n",
    "            CI_upper_ix = int(math.ceil((1.0- CI_dev) * (len(scores[model][metric])-1)))\n",
    "            CI_lower[model][metric]=sorted_scores[CI_lower_ix]\n",
    "            CI_upper[model][metric]=sorted_scores[CI_upper_ix]\n",
    "    \n",
    "    # save results to large  results dictionary\n",
    "    results[exp_name]['performance_scores'] = {'main':main_scores, 'ci_lower': CI_lower, 'ci_upper':CI_upper}\n",
    "    results[exp_name]['performance_scores_all'] = scores\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibration plots (fan plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.calibration\n",
    "import statsmodels.api as sm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 18}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "\n",
    "\n",
    "calib_out_path = './calib_plots/'\n",
    "lowess = sm.nonparametric.lowess\n",
    "n_bins = 10\n",
    "loess_frac=0.33\n",
    "loess_it=3\n",
    "strategy='quantile' # or quantile\n",
    "for exp_name in results: \n",
    "    print(exp_name)\n",
    "    if not os.path.exists(calib_out_path + '/' + exp_name + '/' ):\n",
    "        os.makedirs(calib_out_path + '/' + exp_name + '/')\n",
    "    for model in results[exp_name]['preds']:\n",
    "        all_preds, all_labs = [],[]\n",
    "        print(model)\n",
    "        plot_name = exp_name + ':'+model+'.pdf'\n",
    "        # start plot\n",
    "        plt.axis([0, 1, 0, 1])\n",
    "        slope = 'TBA'\n",
    "        cs = round(results[exp_name]['performance_scores']['main'][model]['Slope'],2)\n",
    "        cs_lower = results[exp_name]['performance_scores']['ci_lower'][model]['Slope']\n",
    "        print(cs_lower)\n",
    "        auroc = round(results[exp_name]['performance_scores']['main'][model]['C (ROC)'],2)\n",
    "        r2 = round(results[exp_name]['performance_scores']['main'][model]['R2'],2)\n",
    "        \n",
    "        plt.text(0.35, .15, 'AUC=%0.2f (%0.2f, %0.2f)' % (results[exp_name]['performance_scores']['main'][model]['C (ROC)'], results[exp_name]['performance_scores']['ci_lower'][model]['C (ROC)'],results[exp_name]['performance_scores']['ci_upper'][model]['C (ROC)']))\n",
    "        plt.text(0.35, 0.05, 'R²=%0.2f (%0.2f, %0.2f)' % (results[exp_name]['performance_scores']['main'][model]['R2'], results[exp_name]['performance_scores']['ci_lower'][model]['R2'],results[exp_name]['performance_scores']['ci_upper'][model]['R2']))\n",
    "\n",
    "        plt.text(0.03, .9, 'Intercept=%0.2f (%0.2f, %0.2f)' % (results[exp_name]['performance_scores']['main'][model]['Intercept'], results[exp_name]['performance_scores']['ci_lower'][model]['Intercept'],results[exp_name]['performance_scores']['ci_upper'][model]['Intercept']))        \n",
    "        plt.text(0.03, .8, 'Slope=%0.2f (%0.2f, %0.2f)' % (results[exp_name]['performance_scores']['main'][model]['Slope'], results[exp_name]['performance_scores']['ci_lower'][model]['Slope'],results[exp_name]['performance_scores']['ci_upper'][model]['Slope']))\n",
    "        \n",
    "        for bs in results[exp_name]['preds'][model]:\n",
    "            prob_true, prob_pred = sklearn.calibration.calibration_curve(results[exp_name]['labs'][bs], results[exp_name]['preds'][model][bs], n_bins=n_bins, strategy=strategy)\n",
    "            \n",
    "            z = lowess(prob_true,prob_pred, frac=loess_frac,it=loess_it)\n",
    "\n",
    "            plt.plot(z[:,0], z[:,1], alpha=0.1, color='k')\n",
    "            all_preds += results[exp_name]['preds'][model][bs]\n",
    "            all_labs += results[exp_name]['labs'][bs]\n",
    "\n",
    "        full_prob_true, full_prob_pred = sklearn.calibration.calibration_curve(all_labs, all_preds, n_bins=2*n_bins, strategy=strategy)\n",
    "        z = lowess(full_prob_true,full_prob_pred, frac=loess_frac,it=loess_it)\n",
    "\n",
    "        plt.plot(z[:,0], z[:,1], alpha=1.0, color='b')\n",
    "        plt.plot(numpy.arange(0,1+1.0/n_bins,1.0/n_bins),numpy.arange(0,1+1.0/n_bins,1.0/n_bins), alpha=1.0, color='r',linestyle='--')\n",
    "\n",
    "        plt.savefig(calib_out_path + '/' + exp_name + '/' + plot_name, bbox_inches='tight')\n",
    "\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> CompareMethods:sim-D CompareMethods:sim-D'\n",
      "> logMSE\n",
      "./boxplots/CompareMethods:sim-D//CompareMethods-sim-D-logMSE.pdf\n",
      ">>> CompareMethods:sim-C CompareMethods:sim-C'\n",
      "> logMSE\n",
      "./boxplots/CompareMethods:sim-C//CompareMethods-sim-C-logMSE.pdf\n",
      ">>> CompareMethods:sim-A CompareMethods:sim-A'\n",
      "> logMSE\n",
      "./boxplots/CompareMethods:sim-A//CompareMethods-sim-A-logMSE.pdf\n",
      ">>> CompareMethods:sim-B CompareMethods:sim-B'\n",
      "> logMSE\n",
      "./boxplots/CompareMethods:sim-B//CompareMethods-sim-B-logMSE.pdf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 20}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "\n",
    "label_map = {'logMSE':'log MSE','MSE':'MSE', 'ExpJacc':'MJICS'}\n",
    "\n",
    "#ylims = {'MSE':(-0.05,.41),'ExpJacc':(0.47,1.0)} # for A & B\n",
    "ylims = {'logMSE':(-7,2),'MSE':(-0.05,1.25),'ExpJacc':(0.25,1.05)} #for C & D\n",
    "\n",
    "exp_pairs = [(a,b) for a in results for b in results if b.replace(\"'\",\"\")==a and a!=b]\n",
    "\n",
    "# real data exps.\n",
    "#exp_pairs = [(\"CompareMethods:real-A\",\"CompareMethods:real-B'\"), (\"CompareMethods:real-C\",\"CompareMethods:real-D'\")]\n",
    "\n",
    "for low_vif_exp, high_vif_exp in exp_pairs:\n",
    "    low_name = low_vif_exp.replace('CompareMethods:sim-','') \n",
    "    high_name = high_vif_exp.replace('CompareMethods:sim-','').replace(\"'\",'$_▵$') #▵\n",
    "\n",
    "    out_path = './boxplots/' + low_vif_exp + '/'\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "        \n",
    "    print('>>>',low_vif_exp, high_vif_exp)\n",
    "    for metric in ['logMSE','ExpJacc']:\n",
    "        \n",
    "        data_dict = {'Model':[], 'Median VIF':[],metric:[]}\n",
    "        print('>', metric)\n",
    "\n",
    "        for model in results[low_vif_exp]['performance_scores_all']:\n",
    "            for value in results[low_vif_exp]['performance_scores_all'][model][metric].values():\n",
    "                    data_dict['Model'].append(model)\n",
    "                    data_dict['Median VIF'].append(low_name)\n",
    "                    data_dict[metric].append(value)\n",
    "                    \n",
    "        for model in results[high_vif_exp]['performance_scores_all']:\n",
    "            for value in results[high_vif_exp]['performance_scores_all'][model][metric].values():\n",
    "                    data_dict['Model'].append(model)\n",
    "                    data_dict['Median VIF'].append(high_name)\n",
    "                    data_dict[metric].append(value)\n",
    "        boxplot_df = pd.DataFrame.from_dict(data_dict)\n",
    "        plt.figure(figsize=(14, 10))\n",
    "\n",
    "        \n",
    "        cmap = matplotlib.cm.get_cmap('Set1')\n",
    "        red = cmap.colors[0]\n",
    "        blue = cmap.colors[1]\n",
    "        legend_elements = [Patch(facecolor=blue, edgecolor='black', label=low_name),Patch(facecolor=red, edgecolor='black', label=high_name)]\n",
    "        bp = sns.boxplot(x=\"Model\", y=metric, hue='Median VIF',linewidth=1, width=.7, data=boxplot_df, palette=[blue,red],fliersize=0, hue_order=[low_name, high_name])\n",
    "        sp = sns.stripplot(x=\"Model\", y=metric, hue='Median VIF', palette=[blue,red],dodge=True, data=boxplot_df, alpha=0.15, hue_order=[low_name, high_name])\n",
    "        sp.legend([],[], frameon=False)\n",
    "\n",
    "        bp.set(xlabel=None)\n",
    "        sp.set(xlabel=None)\n",
    "        bp.set(ylabel=label_map[metric])\n",
    "        \n",
    "        plt.ylim(ylims[metric][0], ylims[metric][1])\n",
    "        plt.legend(handles=legend_elements)\n",
    "\n",
    "        plt.savefig(out_path +'/' + low_vif_exp + ':' + metric +'.pdf', bbox_inches='tight')\n",
    "        print(out_path +'/' + low_vif_exp.replace(':','-') + '-' + metric +'.pdf')\n",
    "        plt.cla()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompareMethods:sim-D\n",
      "CompareMethods:sim-D'\n",
      "CompareMethods:sim-C\n",
      "CompareMethods:sim-C'\n",
      "CompareMethods:sim-A\n",
      "CompareMethods:sim-A'\n",
      "CompareMethods:sim-B\n",
      "CompareMethods:sim-B'\n",
      "      Collin  EPV Hyperparameter        Value     Outcome PredSet\n",
      "0          ▿  2.0         CLasso     2.121903   dysphagia       D\n",
      "1          ▿  2.0         LLasso     0.471275   dysphagia       D\n",
      "2          ▿  2.0         CRidge     0.444094   dysphagia       D\n",
      "3          ▿  2.0         LRidge     2.251776   dysphagia       D\n",
      "4          ▿  2.0        CL1ENet   100.000000   dysphagia       D\n",
      "...      ...  ...            ...          ...         ...     ...\n",
      "10395      ▵  8.0         NC_PCA     5.000000  xerostomia       B\n",
      "10396      ▵  8.0         NC_LAE     4.000000  xerostomia       B\n",
      "10397      ▵  8.0          LAE_C     0.001000  xerostomia       B\n",
      "10398      ▵  8.0          LAE_L  1000.000000  xerostomia       B\n",
      "10399      ▵  8.0             DR     0.500000  xerostomia       B\n",
      "\n",
      "[10400 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "exp_pairs = [(a,b) for a in results for b in results if b.replace(\"'\",\"\")==a and a!=b]\n",
    "\n",
    "params = []\n",
    "hypp_df = pandas.DataFrame({'Collin':[],'EPV':[],'Hyperparameter':[],'Value':[],'Outcome':[],'PredSet':[]})\n",
    "\n",
    "to_float = lambda x: float(x)\n",
    "for low_exp, high_exp in exp_pairs:\n",
    "    for exp_name in [low_exp, high_exp]:\n",
    "        plot_exp_name = exp_name.replace('CompareMethods:sim-','').replace(\"'\",'$_▵$') #▵\n",
    "        predictor_set = plot_exp_name.replace('$_▵$','')\n",
    "        collin = '▵' if '▵' in plot_exp_name else '▿'\n",
    "        print(exp_name)\n",
    "        outcome = 'xerostomia' if ('A' in plot_exp_name or 'B' in plot_exp_name) else 'dysphagia'\n",
    "        no_preds = len(results[exp_name]['gt_models'][0].best_estimator_.coef_[0])\n",
    "        \n",
    "\n",
    "        for bs in results[exp_name]['bs_ixs']: # TODO remove 10\n",
    "            \n",
    "            num_events = sum(results[exp_name]['train_labs'][bs])[0]\n",
    "            epv = round(num_events / no_preds,0)\n",
    "            \n",
    "            trained_models_path = results[exp_name]['bs_path'][bs] + '/trained_models.p'\n",
    "            \n",
    "            if os.path.exists(trained_models_path) and bs in results[exp_name]['gt_models']:\n",
    "                with open(trained_models_path,'rb') as f:\n",
    "\n",
    "                    for model_name, trained_models in pickle.load(f).items():\n",
    "                        for model in trained_models:\n",
    "                            \n",
    "                            if model_name == 'Lasso':\n",
    "                                hypp_df = hypp_df.append({'Outcome':outcome,'PredSet':predictor_set , 'EPV':epv, 'Collin':collin,'Hyperparameter': \"CLasso\",'Value':  to_float(model.best_estimator_.L1_C)},ignore_index=True)\n",
    "                                hypp_df = hypp_df.append({'Outcome':outcome,'PredSet':predictor_set , 'EPV':epv, 'Collin':collin,'Hyperparameter': \"LLasso\",'Value': 1.0 / to_float(model.best_estimator_.L1_C)},ignore_index=True)\n",
    "                            elif model_name == 'Ridge':\n",
    "                                hypp_df = hypp_df.append({'Outcome':outcome,'PredSet':predictor_set, 'EPV':epv, 'Collin':collin, 'Hyperparameter': 'CRidge','Value': to_float(model.best_estimator_.L2_C)},ignore_index=True)\n",
    "                                hypp_df = hypp_df.append({'Outcome':outcome,'PredSet':predictor_set, 'EPV':epv, 'Collin':collin, 'Hyperparameter': 'LRidge','Value': 1.0 / to_float(model.best_estimator_.L2_C)},ignore_index=True)\n",
    "                            elif model_name == 'ElasticNet':\n",
    "                                hypp_df = hypp_df.append({'Outcome':outcome,'PredSet':predictor_set, 'EPV':epv, 'Collin':collin,'Hyperparameter': 'CL1ENet','Value': to_float(model.best_estimator_.L1_C)},ignore_index=True)\n",
    "                                hypp_df = hypp_df.append({'Outcome':outcome,'PredSet':predictor_set, 'EPV':epv, 'Collin':collin,'Hyperparameter': 'CL2ENet','Value':  to_float(model.best_estimator_.L2_C)},ignore_index=True)\n",
    "                                hypp_df = hypp_df.append({'Outcome':outcome,'PredSet':predictor_set, 'EPV':epv, 'Collin':collin,'Hyperparameter': 'CENet','Value':  to_float(model.best_estimator_.L2_C+model.best_estimator_.L1_C)},ignore_index=True)\n",
    "                                hypp_df = hypp_df.append({'Outcome':outcome,'PredSet':predictor_set, 'EPV':epv, 'Collin':collin,'Hyperparameter': 'LENet','Value':  to_float(1.0 / model.best_estimator_.L2_C+ 1.0 / model.best_estimator_.L1_C)},ignore_index=True)\n",
    "                            elif model_name == 'PCLR':\n",
    "                                hypp_df = hypp_df.append({'Outcome':outcome,'PredSet':predictor_set, 'EPV':epv, 'Collin':collin,'Hyperparameter': 'NC_PCA','Value': to_float(model.best_estimator_.n_components)},ignore_index=True)\n",
    "                            elif model_name == 'LAELR':\n",
    "                                hypp_df = hypp_df.append({'Outcome':outcome,'PredSet':predictor_set, 'EPV':epv, 'Collin':collin,'Hyperparameter': 'NC_LAE','Value': to_float(model.best_estimator_.LAE_h)},ignore_index=True)\n",
    "                                hypp_df = hypp_df.append({'Outcome':outcome,'PredSet':predictor_set, 'EPV':epv, 'Collin':collin,'Hyperparameter': 'LAE_C','Value': to_float(model.best_estimator_.LAE_C)},ignore_index=True)\n",
    "                                hypp_df = hypp_df.append({'Outcome':outcome,'PredSet':predictor_set, 'EPV':epv, 'Collin':collin,'Hyperparameter': 'LAE_L','Value': 1.0 / to_float(model.best_estimator_.LAE_C)},ignore_index=True)\n",
    "                            elif model_name == 'Dropout':\n",
    "                                hypp_df = hypp_df.append({'Outcome':outcome,'PredSet':predictor_set, 'EPV':epv, 'Collin':collin,'Hyperparameter': 'DR','Value': to_float(model.best_estimator_.dropout_ratio)},ignore_index=True)\n",
    "\n",
    "\n",
    "                               \n",
    "print(hypp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xerostomia\n",
      "./hypperparamplots//xerostomia:LRidge-LLasso-LENet.pdf\n",
      "./hypperparamplots//xerostomia:CRidge-CLasso-CENet.pdf\n",
      "./hypperparamplots//xerostomia:DR.pdf\n",
      "./hypperparamplots//xerostomia:NC_LAE-NC_PCA.pdf\n",
      "dysphagia\n",
      "./hypperparamplots//dysphagia:LRidge-LLasso-LENet.pdf\n",
      "./hypperparamplots//dysphagia:CRidge-CLasso-CENet.pdf\n",
      "./hypperparamplots//dysphagia:DR.pdf\n",
      "./hypperparamplots//dysphagia:NC_LAE-NC_PCA.pdf\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pathsep + '/usr/bin')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "cmap = matplotlib.cm.get_cmap('Set1')\n",
    "red = cmap.colors[0]\n",
    "blue = cmap.colors[1]\n",
    "from matplotlib import rc\n",
    "\n",
    "matplotlib.rcParams['text.usetex'] = False\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 20}\n",
    "matplotlib.rc('font', **font)\n",
    "out_path = './hypperparamplots/' \n",
    "if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "for outcome in ['xerostomia','dysphagia']:\n",
    "    print(outcome)\n",
    "\n",
    "    label_map = {'DR':'$\\delta$','CRidge':'$c_{\\ell_2}$','CLasso':'$c_{\\ell_1}$','NC_PCA':'$d_{PCA}$','NC_LAE':'$d_{LAE}$','CENet':'$c_{ENet}$','LRidge':'$\\lambda_{\\ell_2}$','LLasso':'$\\lambda_{\\ell_1}$','LENet':'$\\lambda_{ENet}$'}\n",
    "    #label_map = {'LRidge':'$\\lambda_{\\ell_2}$','LLasso':'$\\lambda_{\\ell_1}$','LENet':'$\\lambda_{ENet}$'}\n",
    "    \n",
    "    groups = [['LRidge',\"LLasso\",\"LENet\"],['CRidge',\"CLasso\",\"CENet\"],['DR'],['NC_LAE','NC_PCA']]\n",
    "    sel_outcomes = [outcome]\n",
    "    \n",
    "    p = {'▵':red, '▿':blue}\n",
    "    for selection in groups:\n",
    "        f, ax = plt.subplots(figsize=(9, 9))\n",
    "        lp = sns.lineplot(legend='brief',\n",
    "        data=hypp_df[hypp_df['Hyperparameter'].isin(selection) &  hypp_df['Outcome'].isin(sel_outcomes)], ax=ax, x=\"PredSet\", y=\"Value\", style=\"Hyperparameter\", hue=\"Collin\", hue_order=['▵', '▿'], err_style='bars',err_kws={'capsize':5,'elinewidth':1}, ci=95, palette=[red, blue])                \n",
    "\n",
    "        lp.set(xlabel=None,ylabel=None)\n",
    "        leg = ax.legend()\n",
    "        new_labels=[label_map[l] if l in label_map else l for l in selection]\n",
    "        old_leg_handles = leg.legendHandles[-len(selection):]\n",
    "        plt.legend(old_leg_handles, new_labels, title=\"\")\n",
    "        \n",
    "        plt.savefig(out_path +'/' + outcome + ':' + '-'.join(selection) +'.pdf', bbox_inches='tight')\n",
    "        print(out_path +'/' + outcome + ':' + '-'.join(selection) +'.pdf')\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficient evaluation tables (LaTeX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_oar_coef(coef_name):\n",
    "    for oar_substring in oar_coeff_substrings:\n",
    "        if oar_substring in coef_name:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for exp_name in results:\n",
    "    print(exp_name)    \n",
    "    if 'umcg-cv' in exp_name: # Only include external evaluation settings for coefficient evaluation.\n",
    "        print('...ignored')\n",
    "        continue\n",
    "        \n",
    "    # 1. calculate summarizing coefficient statistics\n",
    "    coeff_metrics = {model:{metric:{} for metric in coeff_metrics_to_include} for model in models_to_include}\n",
    "    for bs in results[exp_name]['bs_ixs']:\n",
    "        for model in coeff_metrics:\n",
    "            first_coef_name = list(results[exp_name]['models'][model].keys())[0]\n",
    "            if bs in results[exp_name]['models'][model][first_coef_name]:\n",
    "                oar_coef_values = [results[exp_name]['models'][model][coef_name][bs] for coef_name in results[exp_name]['models'][model] if is_oar_coef(coef_name)]\n",
    "                neg_oar_coef_values = [v for v in oar_coef_values if v < -0.01]\n",
    "                pos_oar_coef_values = [v for v in oar_coef_values if v > 0.01]\n",
    "                \n",
    "                if 'sum_neg_oar' in coeff_metrics_to_include:\n",
    "                    coeff_metrics[model]['sum_neg_oar'][bs] = sum(neg_oar_coef_values)\n",
    "                if 'sum_pos_oar' in coeff_metrics_to_include:\n",
    "                    coeff_metrics[model]['sum_pos_oar'][bs] = sum(pos_oar_coef_values)\n",
    "                if 'perc_oar_neg' in coeff_metrics_to_include:\n",
    "                    coeff_metrics[model]['perc_oar_neg'][bs] = len(neg_oar_coef_values) / len(oar_coef_values)\n",
    "                if 'perc_oar_pos' in coeff_metrics_to_include:\n",
    "                    coeff_metrics[model]['perc_oar_pos'][bs] = len(pos_oar_coef_values) / len(oar_coef_values)\n",
    "                    \n",
    "    # 2. Determine the reported scores, and confidence intervals\n",
    "    main_scores, CI_lower, CI_upper = {model:{} for model in models_to_include}, {model:{} for model in models_to_include}, {model:{} for model in models_to_include}\n",
    "    for model in coeff_metrics:\n",
    "        for metric in coeff_metrics_to_include:\n",
    "            sorted_scores = sorted(coeff_metrics[model][metric].values())\n",
    "            main_scores[model][metric] = numpy.mean(list(coeff_metrics[model][metric].values()))\n",
    "            \n",
    "            CI_lower_ix = int(CI_dev*len(coeff_metrics[model][metric]))\n",
    "            CI_upper_ix = int(math.ceil((1.0- CI_dev) * (len(coeff_metrics[model][metric])-1)))\n",
    "            CI_lower[model][metric]=sorted_scores[CI_lower_ix]\n",
    "            CI_upper[model][metric]=sorted_scores[CI_upper_ix]\n",
    "        for coef_name in results[exp_name]['models'][model]:\n",
    "            sorted_scores = sorted(results[exp_name]['models'][model][coef_name].values())\n",
    "\n",
    "            main_scores[model][coef_name] = numpy.mean(list(results[exp_name]['models'][model][coef_name].values()))\n",
    "            CI_lower_ix = int(CI_dev*len(results[exp_name]['models'][model][coef_name]))\n",
    "            CI_upper_ix = int(math.ceil((1.0- CI_dev) * (len(results[exp_name]['models'][model][coef_name])-1)))\n",
    "            CI_lower[model][coef_name]=sorted_scores[CI_lower_ix]\n",
    "            CI_upper[model][coef_name]=sorted_scores[CI_upper_ix]            \n",
    " \n",
    "    \n",
    "    # save results to large  results dictionary\n",
    "    results[exp_name]['coeff_scores'] = {'main':main_scores, 'ci_lower': CI_lower, 'ci_upper':CI_upper}\n",
    "    results[exp_name]['coeff_scores_all'] = coeff_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write score tables (LaTeX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for exp_name in results:\n",
    "    \n",
    "    # Setup output dir\n",
    "    out_file = top_level_out_dir + '/score_tables_tex/' + exp_name + '.tex'\n",
    "    if not os.path.exists(os.path.dirname(out_file)):\n",
    "        os.makedirs(os.path.dirname(out_file))\n",
    "        \n",
    "    # Write the results in a .tex table (only rows)\n",
    "    score_formatting = lambda x: \"{0:.2f}\".format(round(x, 2))\n",
    "    column_header_formatting = lambda x: x.replace('_mean','').replace('_','.')\n",
    "    ci_formatting = lambda ci: \"$^{(\" + \"{0:.2f}\".format(round(ci[0], 2)) + \", \" + \"{0:.2f}\".format(round(ci[1], 2)) + \")}$\"\n",
    "\n",
    "    with open(out_file, 'w') as f:\n",
    "        all_metrics_to_include = metrics_to_include #+coeff_metrics_to_include    \n",
    "        lines = \"Model\" + '&' + '&'.join([column_header_formatting(metric) for metric in all_metrics_to_include]) + '\\\\\\\\\\n'\n",
    "        for model in models_to_include:\n",
    "            perf_score_content = [score_formatting(results[exp_name]['performance_scores']['main'][model][metric]) + ci_formatting((results[exp_name]['performance_scores']['ci_lower'][model][metric],results[exp_name]['performance_scores']['ci_upper'][model][metric])) for metric in metrics_to_include]\n",
    "            #coeff_score_content = [score_formatting(results[exp_name]['coeff_scores']['main'][model][metric]) + ci_formatting((results[exp_name]['coeff_scores']['ci_lower'][model][metric],results[exp_name]['coeff_scores']['ci_upper'][model][metric])) for metric in coeff_metrics_to_include]\n",
    "            all_score_content = perf_score_content #+ coeff_score_content\n",
    "            lines += model + '&' + '&'.join(all_score_content) + '\\\\\\\\\\n'\n",
    "        f.write(lines)\n",
    "    print('written',out_file)       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficient tables (LaTeX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for exp_name in results:\n",
    "    print(exp_name)\n",
    "        \n",
    "    # Setup output dir\n",
    "    out_file = top_level_out_dir + '/coeff_tables/' + exp_name + '.tex'\n",
    "    if not os.path.exists(os.path.dirname(out_file)):\n",
    "        os.makedirs(os.path.dirname(out_file))\n",
    "        \n",
    "    # Write summarizing statistics and actual coefficients to tables\n",
    "    column_header_formatting = lambda x: '\\multicolumn{2}{l}{' +x.replace('_mean','') +'}'\n",
    "    ci_formatting = lambda ci: \"&$^{(\" + \"{0:.2f}\".format(round(ci[0], 2)) + \", \" + \"{0:.2f}\".format(round(ci[1], 2)) + \")}$\"\n",
    "    score_formatting = lambda x: \"{0:.2f}\".format(round(x, 2))\n",
    "    row_formatting = lambda x: x.replace('X_','').replace('_mean','').replace('_','.').replace('Dmean','Dm').replace('UMCGshortv2','').replace('Submandibular','Subm')\n",
    "    \n",
    "    coef_names = results[exp_name]['models'][models_to_include[0]]\n",
    "    with open(out_file, 'w') as f:\n",
    "        lines = \"\\\\begin{tabular}{lRlRlRlRlRlRlRlRlRlRlRlRlRlRlRlRlR}\\\\toprule\\n\"\n",
    "        lines += \" \" + '&' + '&'.join([column_header_formatting(model) for model in models_to_include]) + '\\EndTableHeader' + '\\\\\\\\\\n'\n",
    "        lines += \"\\\\midrule \"\n",
    "        for coef_metric in coeff_metrics_to_include:\n",
    "            lines += row_formatting(coef_metric)+ '&' + '&'.join([score_formatting(results[exp_name]['coeff_scores']['main'][model][coef_metric]) + ci_formatting((results[exp_name]['coeff_scores']['ci_lower'][model][coef_metric],results[exp_name]['coeff_scores']['ci_upper'][model][coef_metric])) for model in models_to_include]) + '\\\\\\\\\\n'\n",
    "            \n",
    "        lines += \"\\\\midrule \"\n",
    "        for coef_name in coef_names: #results[exp_name]['models'].keys():\n",
    "            lines += row_formatting(coef_name) + '&' + '&'.join([score_formatting(results[exp_name]['coeff_scores']['main'][model][coef_name]) + ci_formatting((results[exp_name]['coeff_scores']['ci_lower'][model][coef_name],results[exp_name]['coeff_scores']['ci_upper'][model][coef_name])) for model in models_to_include]) + '\\\\\\\\\\n'\n",
    "        lines += \"\\\\bottomrule\\n\\\\end{tabular}\"\n",
    "        f.write(lines)\n",
    "    print('written',out_file)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(top_level_out_dir +'/results.p', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "    print('saved results to', top_level_out_dir +'/results.p','\\nDone!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Combined calibration plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import plot_calib_curves\n",
    "\n",
    "\n",
    "for exp_name in results:\n",
    "\n",
    "    # Setup output dir\n",
    "    plot_path = top_level_out_dir + '/combined_calib_plots/' + exp_name + '.pdf'\n",
    "    if not os.path.exists(os.path.dirname(plot_path)):\n",
    "        os.makedirs(os.path.dirname(plot_path))    \n",
    "    \n",
    "    probs_per_model = {model: results[exp_name]['preds'][model][0] for model in results[exp_name]['preds'] if model in models_to_include}\n",
    "    true_labs = results[exp_name]['labs'][0]\n",
    "    \n",
    "    plot_calib_curves(probs_per_model, true_labs, plot_path, n_bins=10, dpi=800)\n",
    "    print('written',plot_path)\n",
    "\n",
    "print('Done.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exps.compare_methods_citor import prep_citor_data\n",
    "from lib.CITOR import CITOR, load_python_object_encrypted\n",
    "from lib.experiment import encode_variables, select_train_test_patients\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "pwd='PWD'\n",
    "\n",
    "score_formatting = lambda x: \"{0:.2f}\".format(round(x, 2))\n",
    "\n",
    "header_formatting = lambda x: x.replace('X_','').replace('_mean','').replace('_','.').replace('Dmean','Dm').replace('UMCGshortv2','').replace('Submandibular','Subm').replace('..','.')\n",
    "column_header_formatting = lambda x: '\\\\rotatebox{270}{\\\\multicolumn{1}{l}{' + header_formatting(x) + '}}'\n",
    "row_header_formatting = header_formatting\n",
    "\n",
    "score_coloring = lambda x: '\\\\multicolumn{1}{E}{' + x + '}'\n",
    "\n",
    "for exp_name in results:\n",
    "    used_data = prep_citor_data(load_python_object_encrypted(results[exp_name]['yaml']['data_path'],pwd))\n",
    "    df, x_names, y_name = encode_variables(used_data, results[exp_name]['yaml'])\n",
    "    df_unlabeled_train_indices, df_train_indices, df_unlabeled_test_indices, df_test_indices = select_train_test_patients(df, results[exp_name]['yaml'], y_name, x_names, bs=0)\n",
    "    \n",
    "    \n",
    "    print(exp_name, len(x_names),len(df_train_indices), len(df_test_indices))\n",
    "    corr_matrix = df.iloc[df_train_indices][x_names].corr()\n",
    "    \n",
    "    flattened_corr_matrix = corr_matrix.values.flatten()\n",
    "    num_pairs_r_squared_larger_than_70_percent = (sum([1.0 if abs(v) > 0.7 else 0 for v in flattened_corr_matrix]) - len(x_names)) / 2\n",
    "\n",
    "    print('no pairs |r| > 0.7:', num_pairs_r_squared_larger_than_70_percent,'/',len(flattened_corr_matrix),'=', num_pairs_r_squared_larger_than_70_percent/len(flattened_corr_matrix) )\n",
    "    plt.matshow(corr_matrix)\n",
    "    plt.show()\n",
    "\n",
    "    # Setup output dir\n",
    "    out_file = top_level_out_dir + '/correlation_tables/' + exp_name + '.tex'\n",
    "    if not os.path.exists(os.path.dirname(out_file)):\n",
    "        os.makedirs(os.path.dirname(out_file))\n",
    "    \n",
    "    \n",
    "    with open(out_file, 'w') as f:\n",
    "        string = \"\\\\begin{tabular}{l|c\" + (len(x_names) * 'c')+ '} \\n\\EndTableHeader \\\\\\\\ \\n'\n",
    "        \n",
    "        for i, i_name in enumerate(x_names): # rows\n",
    "            if i == 0:\n",
    "                continue\n",
    "                \n",
    "            string += header_formatting(i_name) + '&&' \n",
    "            for j,j_name in enumerate(x_names): # columns\n",
    "                if i > j:\n",
    "                    string += score_coloring(score_formatting(corr_matrix.values[i,j])) + '&'\n",
    "                else:\n",
    "                    string += '&'\n",
    "            string += '\\\\\\\\ \\n'\n",
    "            \n",
    "\n",
    "        string += '\\\\midrule&\\\\textcolor{white}{.}&'+'&'.join([column_header_formatting(x_name) for x_name in x_names[:-1]]) + '\\n'\n",
    "            \n",
    "        string += \"\\end{tabular} \\n\"\n",
    "                \n",
    "             \n",
    "        f.write(string)        \n",
    "    \n",
    "    print('written', out_file)\n",
    "    \n",
    "print('Done.')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the baseline table with patient characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xer_indices_umcg, xer_indices_maas = [],[]\n",
    "dys_indices_umcg, dys_indices_maas = [],[]\n",
    "y_names = set()\n",
    "\n",
    "for exp_name in results:\n",
    "    print('>>>',exp_name)\n",
    "    used_data = prep_citor_data(load_python_object_encrypted(results[exp_name]['yaml']['data_path'],pwd))\n",
    "    df, x_names, y_name = encode_variables(used_data, results[exp_name]['yaml'])\n",
    "    df_unlabeled_train_indices, df_train_indices, df_unlabeled_test_indices, df_test_indices = select_train_test_patients(df, results[exp_name]['yaml'], y_name, x_names, bs=0)\n",
    "    \n",
    "    \n",
    "    print('No pts. in total dataset', len(used_data))\n",
    "    print('No pts. with M6 outcomes present', len(df_train_indices) + len(df_test_indices))\n",
    "    \n",
    "    print('UMCG data: n=',len(df_train_indices),'\\n')\n",
    "\n",
    "    if 'xer' in exp_name:\n",
    "        xer_indices_umcg, xer_indices_maas = df_train_indices, df_test_indices\n",
    "    if 'dys' in exp_name:\n",
    "        dys_indices_umcg, dys_indices_maas = df_train_indices, df_test_indices  \n",
    "    \n",
    "    y_names.add(y_name)\n",
    "umcg_both = [i for i in xer_indices_umcg if i in dys_indices_umcg]    \n",
    "maas_both = [i for i in dys_indices_maas if i in xer_indices_maas]    \n",
    "\n",
    "print('UMCG with both outcomes', len(umcg_both))\n",
    "print('UMCG with either outcome', len(set(xer_indices_umcg + dys_indices_umcg)))\n",
    "print('MAAS with both outcomes', len(maas_both))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# To print: AGE (mean, std), GESLACHT (male & female), LOCTUM_cat (quantities per category)\n",
    "\n",
    "for hosp_df, hosp_name in [(used_data.iloc[umcg_both],'UMCG'),(used_data.iloc[maas_both],'MAAS')]:\n",
    "    print('\\n\\n',5*'=',hosp_name, 5*'=')\n",
    "    print('AGE\\n', 'Median',hosp_df['AGE'].median(), '\\n Std.',hosp_df['AGE'].std())\n",
    "    \n",
    "    for k in ['GESLACHT','LOCTUM_cat','MODALITY','TCAT','NCAT'] +list(y_names):\n",
    "        print(k)\n",
    "        vk = hosp_df[k].value_counts()\n",
    "        pk = hosp_df[k].value_counts(normalize=True)\n",
    "        print(pandas.concat([vk,pk], axis=1, keys=['counts', '%']).sort_index())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}